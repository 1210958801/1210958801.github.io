---
layout: post
title: 哈喽Spark
tags: [大数据]
---

![](https://user-gold-cdn.xitu.io/2018/9/18/165ea911855d3d5e?imageView2/1/w/1304/h/734/q/85/format/webp/interlace/1)
#### Spark 是 UC Berkeley AMP lab 所开源的类 Hadoop MapReduce 的通用并行框架，拥有众多丰富的算子以及shell编程提供基于Python,Scala,Java,和SQL的简单易用的API以及内建的丰富的程序库以外，Spark还能和其他大数据工具密切配合使用。
#### 是专为大规模数据处理而设计的快速通用的大数据处理引擎及轻量级的大数据处理统一平台。
#### 当我们在谈 Spark 的时候可能是指一个 Spark 应用程序，替代 MapReduce 运行在 Yarn上，存储在 HDFS 上的一个大数据批处理程序；也可能是指使用包含 Spark sql、Spark streaming 等子项目；甚至 Tachyon、Mesos 等大数据处理的统一平台，或者称为 Spark 生态。

### Spark生态圈
![](https://user-gold-cdn.xitu.io/2018/9/18/165ea8b5dca64259?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)
#### Spark SQL 提供 HiveQL（通过 Apache Hive 的 SQL 变体 Hive 查询语言）与Spark 进行交互的 API。每个数据库表被当做一个 RDD，Spark SQL 查询被转换为 Spark 操作。上篇说到的Spark Streaming 会对实时数据流进行处理和控制，它允许程序能够像普通 RDD 一样处理实时数据。
#### 接下来的系列文章将会详细介绍 Spark 生态中的其他模块与各个子项目，而且会穿插的将子项目能够用到的服务讲一遍，比如:Hive,Kafka,HBase。
## Spark 的基本原理
#### 上边说到Spark既然是用来替代MR的，所以对于两者之间的对比以及Spark基本的概念我们在上一篇文章《离线、批量、实时、流式》已经讲的很细了。如果有不知道的，还请移步。所以我们这章的主要内容是对Spark的原理和Spark实例进行阐述。
![](https://user-gold-cdn.xitu.io/2018/9/18/165ea8b5dedfbf5b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

## RDD

#### RDD是弹性分布式数据集， 是分布式内存的一个抽象概念，RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，只能通过在其他RDD执行确定的转换操作（如map、join和groupby）而创建，然而这些限制使得实现容错的开销很低。对开发者而言，RDD可以看作是Spark的一个对象，它本身运行于内存中，如读文件是一个RDD，对文件计算是一个RDD，结果集也是一个RDD ，不同的分片、 数据之间的依赖 、key-value类型的map数据都可以看做RDD。

#### Spark中最基本，也是最重要的就是RDD(Resilient Distributed Dataset) RDD 其实就是分布式的元素集合。在 Spark 中，对数据的所有操作不外乎创 建 RDD、转化已有 RDD 以及调用 RDD 操作进行求值。而在这一切背后，Spark 会自动将 RDD 中的数据分发到集群上，并将操作并行化执行。

#### Spark 中的 RDD 就是一个不可变的分布式对象集合。每个 RDD 都被分为多个分区，这些 分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。

#### 用户可以使用两种方法创建 RDD:读取一个外部数据集，或在驱动器程序里分发驱动器程 序中的对象集合(比如 list 和 set)。而Spark中的RDD操作不外乎两种：转化操作、行动操作

### Spark中基本的转化操作
![](https://user-gold-cdn.xitu.io/2019/4/14/16a1aeac05cfdc65?w=2030&h=1678&f=png&s=557554)
### Spark中基本的行动操作
![](https://user-gold-cdn.xitu.io/2019/4/14/16a1aeba71774b00?w=2028&h=1630&f=png&s=491048)
#### 以上图片参考来源于:《Spark快速大数据分析》
***
## 安装Spark(单机版)

#### 一如既往,还是先安装单机版本的,之后会补上集群版本的[官方](https://spark.apache.org/downloads.html)瞅着最高版本的就下吧，还得需要注意Spark的版本要和Hadoop的版本相匹配,因为他得用HDFS嘛。

#### 安装完之后还是和之前安装的服务一样,先配置环境变量,然后执行以下命令目的是配置/conf文件夹下的env启动文件

```
cd /usr/local/spark
cp ./conf/spark-env.sh.template ./conf/spark-env.sh 
chmod 755 spark-env.sh #将文件的权限设置为可执行
```
#### 然后需要在这个文件下添加如下

```
export JAVA_HOME=/usr/local/jdk1.8 #Java的环境变量
export SPARK_MASTER_IP=192.168.20.88 #spark主机的地址
export SPARK_WORKER_MEMORY=8g #从机的内存
```
#### 注意如果要使用HDFS请在启动之前先启动Hadoop
### 启动之后的效果
![](https://user-gold-cdn.xitu.io/2019/4/2/169dbca263c40f82?w=3834&h=1182&f=png&s=265230)
#### 注意端口号和Tomcat冲突,改一个即可。

## 安装Spark(集群版)
#### 敬请期待~
***
## WordCount实例

#### 虽然Spark官方支持很多语言:Scala/Java/Python/R/SQL内置函数但是Spark还是特别希望我们用Scala进行编程的,所以我们先来整一个Scala版本的实例。先从基本的配置开始

#### 依然是[scala官方链接](https://www.scala-lang.org/download/)、[SBT官方链接](https://www.scala-sbt.org/download.html)介绍一下SBT:SimpleBuildTools一语中的,明白了吗？当然我们用Maven也可以。一样的道理。

#### 然后需要在IDEA中搜索插件:Scala。安装=>重启=>NewProject=>然后选择Scala

![](https://user-gold-cdn.xitu.io/2019/4/2/169dc1632bb23c14?w=2174&h=1382&f=png&s=184388)
#### 基本的项目结构
![](https://user-gold-cdn.xitu.io/2019/4/2/169dc183c2cd2f2e?w=1000&h=746&f=png&s=80012)
#### 然后需要加入Spark的依赖。
![](https://user-gold-cdn.xitu.io/2019/4/2/169dc20b008ddf85?w=3840&h=2114&f=png&s=383562)

```
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.4.0" 
```
#### 然后新建Scala文件输入以下代码然后导入包就行了。

```
object WordCount {
  def main(args: Array[String]) {
    val conf = new SparkConf()
    conf.setAppName("WordCount")
    val sc = new SparkContext(conf)

    val file = "hdfs://192.168.20.88:9000/SparkFile/wordcount.txt" # HDFS的访问路径
    val lines = sc.textFile(file)
    val words = lines.flatMap(_.split(" "))
    words.saveAsTextFile("/usr/local/fileSpark")
  }
}
```
#### 最后就剩下打包了，SBT打包不像Maven有插件自己打包。所以我们必须按照以下步
![](https://user-gold-cdn.xitu.io/2019/4/2/169dca192571831d?w=2560&h=2082&f=png&s=265723)
![](https://user-gold-cdn.xitu.io/2019/4/2/169dca28a043f59e?w=1300&h=888&f=png&s=260150)
![](https://user-gold-cdn.xitu.io/2019/4/2/169dca2ef12f2868?w=1034&h=746&f=png&s=91813)
#### 将项目内其他依赖全部删掉
![](https://user-gold-cdn.xitu.io/2019/4/2/169dca493f53c7df?w=2560&h=2082&f=png&s=530649)
#### 只留下这一个
![](https://user-gold-cdn.xitu.io/2019/4/2/169dca5338d082ba?w=2560&h=2082&f=png&s=267521)
#### 之后就会生成一个要打包内容的文件
![](https://user-gold-cdn.xitu.io/2019/4/2/169dca58a7a291f8?w=1030&h=926&f=png&s=90221)
#### 然后build Artifacts=>build就可以了
![](https://user-gold-cdn.xitu.io/2019/4/2/169dca6f58897937?w=3834&h=2110&f=png&s=614758)
#### 然后放到服务器上,前提是要将HDFS和Spark服务起来。然后执行(需要注意一点的是,你可能会出现版本不一致的问题，因为spark2.4依赖的是scala2.11.12。而且你看他的jars文件夹下关于scala的包是2.11.12的，所以你需要在sbt的依赖配置文件中加入⬇️所以说这个就很坑了，谁能想到最新的spark他不支持最新的scala)

```
scalaVersion := "2.11.12"
```
```
spark-submit --class com.shade.spark.WordCount --master local ./SimpleSpark.jar #当然你的项目包内容和打包后的位置需要改变。
```
#### 然后最终的结果就会输出出来了。
![](https://user-gold-cdn.xitu.io/2019/4/2/169dcbcded89b9ef?w=782&h=74&f=png&s=13356)