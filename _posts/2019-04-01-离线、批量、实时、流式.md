---
layout: post
title: 离线、批量、实时、流式
tags: [大数据]
---

### 数据时代只有数据的存储当然不够，如何从数据中获取业务需要的信息才能创造价值，这类工作就需要计算框架来完成。先来推翻一个驳论。

## 离线=批量？实时=流式？
#### 习惯上我们认为离线和批量等价；实时和流式等价，但其实这种观点并不完全正确。假设一种情况：当我们拥有一个非常强大的硬件系统，可以毫秒级的处理Gb级别的数据，那么批量计算也可以毫秒级得到统计结果（当然这种情况非常极端，目前不可能），那我们还能说它是离线计算吗？
#### 所以说离线和实时应该指的是：数据处理的延迟；批量和流式指的是：数据处理的方式。两者并没有必然的关系。事实上SparkStreaming就是采用小批量（batch）的方式来实现实时计算。而MapReduce和Spark则属于离线计算
#### 所有下次有人告诉你离线=批量、实时=流式的时候这段话能够帮到你。下面开始今天的主题...

## 离线计算
#### 所谓的离线计算就是在计算开始前已知所有输入数据，输入数据不会产生变化。举个例子:比如Boss第二天要看头一天的财务报表,财务报表的数据存在Hive(业务数据)当中,然后你做一个定时任务,每天五点的时候去计算昨天的数据。从而将得到的结果再放回到Hive中然后通过Hive数据源取到已经做好计算的数据,然后渲染到页面上。这样一整个过程其实就是离线计算。也就是说我们在拿到数据之后并不着急要得到结果。如此类型的都归于离线计算。
### MapReduce
#### MapReduce是离线批量计算的代表，采用移动计算优于移动数据的理念，计算任务通常直接在HDFS的datanode上运行，这样避免了数据的移动（当然reduce阶段还是需要节点间传输数据），并且采用并行计算的方式，大大减少了数据处理时间。(具体的计算方式查阅上篇,不赘述)
![](https://user-gold-cdn.xitu.io/2019/3/29/169c7254b4e6cb7c)
#### 但是map任务在处理数据后，会根据reduce的个数，生成对应个数中间文件，这些文件保存在磁盘上，map计算完成后reduce将文件fetch过来进行汇总。也就是说如果采用MR，无论输入源数据有多少，哪怕内存放得下，中间结果还是需要落盘，而我们知道io是计算机中最耗时的操作，这也是造成MR效率较低的一个原因。
### 从上面的介绍可以看出MR存在两个缺点：
#### 1.无论源数据有多少，shuffle过程的中间数据都需要落盘，效率较低，不能很好的利用内存。
#### 2.MapReduce只提供了map和reduce函数，api不够友好，编写代码时，首先都需要将计算思想转化成MapReduce模型，<span style="color:red">非常反人类</span>（写过MR的应该非常有感触），而且稍微复杂点的计算需要多次迭代（尤其现在都在提人工智能，深度学习，这些都是比较复杂的逻辑计算），迭代启动任务也有较大的开销。这也就是上一篇的时候我着重强调,这个东西了解其运行的步骤就可以了。

### Spark
#### Spark也是一个批量计算框架,Spark主要对MapReduce中的这两点进行了优化，我们来看一下Spark的运行逻辑,之后我们会对整个spark进行详细的阐述。
![](https://user-gold-cdn.xitu.io/2018/9/18/165ea8b5dcb99930)
#### 首先我们可以看到 Spark 提供了丰富的算子（textFile、FlatMap、Map、ReduceByKey 等），在计算的中间结果也没有存储到 HDFS 的操作。Spark的将数据抽象成RDD，[RDD](https://baike.baidu.com/item/RDD/5840158)：弹性分布式数据集（Resilient DistributedDatasets），这是一种分布式的内存抽象，允许在大型集群上执行基于内存的计算，与此同时还保持了MapReduce等数据流模型的容错特性。RDD可以常驻的内存的属性，大大简化了迭代计算所需的开销，Spark任务可以立马利用上一次计算出来的RDD来进行下次迭代。
### Spark的优点
#### 1.它不止提供了map和reduce逻辑，还提供了额外的api，更好的支持了复杂的迭代计算，如：groupby、join等函数。
#### 2.更好的利用内存：内存够用的情况下，数据直接在内存进行处理，减少数据落盘次数，提高计算效率。
#### 这时候问题来了：那既然Spark这么好，是不是Spark可以完全取代MR了？
#### 答：也没有那么绝对，任何东西都有它存在的理由，都有它发光发热的地方，比如当数据量非常大，Spark内存放不下的时候，数据也是需要落盘的。对海数据简单清洗，排序时，Spark的性能并不一定就比MR好，所以还需要看具体的业务。
#### 而且前两年大家都在说Spark将会取代Hadoop，与其说是取代Hadoop，倒不如说Spark有可能取代MapReduce，因为Spark自身是没有存储功能的，数据源往往依赖HDFS存储。MR和Spark采用批量的方式解决离线计算业务，那想象一下另一些场景：在金融业务中，我们希望能够马上检测到有问题的账单，或者我们希望实时得到系统状态的统计信息，若采用批量框架，很难及时的得到反馈结果，这时就需要实时计算。

#### MR和Spark的对比如下:
![](https://user-gold-cdn.xitu.io/2018/9/18/165ea8b5dcd71a32)

### RDD优化
#### 需要注意的是内存不够用时RDD也可以采用磁盘作为其第二存储介质，而且如果计算中包含shuffle过程，为了保证容灾，shuffle处的RDD也是需要落盘的。Spark为了提高效率，采取了如下优化：当下游的RDD和上游的RDD是NarrowDependency时（也就是上游partition和下游partition是1对1，或n对1的关系，不涉及shuffle逻辑，比如map，flatmap等操作），将这些RDD放在一个task里面执行，大大减少了计算时所需的内存开销，也减少了网络传输的数据量，详细可以参考。(很多计算框架都有采用这种优化，比如Flink)

## 实时计算
### Storm
#### Storm做为最早的一个实时计算框架，早期应用于各大互联网公司
![](https://user-gold-cdn.xitu.io/2019/3/30/169cdb423fd03766)
#### spout：负责从数据源接收数据bolt：负责数据处理，最下游的bolt负责数据输出spout不断从数据源接收数据，然后按一定规则发送给下游的bolt进行计算，最下游的bolt将最终结果输出到外部系统中（这里假设输出到DB），这样我们在DB中就可以看到最新的数据统计结果。Storm每一层的算子都可以配置多个，这样保证的水平扩展性。因为往往处理的是unbound data，所以storm中的算子都是长任务。
### 容灾
#### 批处理解决方案就比较简单，拿MR举例，假如一个运行中map或reduce失败，那么任务重新提交一遍就ok（只不过重头计算又要花费大量时间）。
#### 下面我们看看Storm是如何解决的：storm的spout有一个buffer，会缓存接收到的record，并且Storm还有一个acker（可以认为是一个特殊的bolt任务），每条record和该record所产生的所有tuple在处理完成后都会向对应的acker发送ack消息，当acker接收到该record所有的ack消息之后，便认为该record处理成功，并通知spout从buffer中将该record移除，若receiver没有在规定的时间内接收到ack，acker则通知spout重放数据。
![](https://user-gold-cdn.xitu.io/2019/3/30/169cdc3a02719ada)
##### acker个数可以由用户指定，因为数据量比较大时，一个acker可能处理不过来所有的ack信息，成为系统瓶颈（如果可以容忍数据丢失，当然也可以关闭ack机制，可以显著提高系统性能）。并且acker采用了巧妙的机制，优化了ack机制的资源占用（有兴趣的同学可以参考官网，网上也有很多博客介绍ack具体实现）。Storm采用ack机制实现了数据的重放，尽管做了很多优化，但是毕竟每条record和它产生的tuple都需要ack，对吞吐还是有较大的影响，关闭ack的话，对于某些不允许丢数据的业务来说又是不可接受的。
#### 但是还有问题需要我们注意:
![](https://user-gold-cdn.xitu.io/2019/3/30/169cdc7511c9c46d)
#### 如果说按照上边所说的storm容灾方法,那么如果blotA已经处理完了，已经将结果提交给DB了但是blotB却挂了,然后数据重放,那问题就来了，第一次boltA进入DB的数据会再次进入，如何在保证了数据不丢的前提下保证不重复。
#### 不仅sink处存在重复输出的问题，receiver处也同样存在这种问题。因为上边我们讲到在规定的时间内，如果receiver没有给acker消息就会信息重放，但是如果receiver没有挂那？那信息就是不是重复了。

#### Storm没有提供exactly once的功能，并且开启ack功能后又会严重影响吞吐，所以会给大家一种印象：流式系统只适合吞吐相对较小的、低延迟不精确的计算；而精确的计算则需要由批处理系统来完成，所以出现了Lambda架构，该架构由Storm的创始人提出，简单的理解就是同时运行两个系统：一个流式，一个批量，用批量计算的精确性来弥补流式计算的不足，但是这个架构存在一个问题就是需要同时维护两套系统，代价比较大。那么有没有一种架构，可以满足高吞吐、低延迟的要求，同时也提供exactly once功能？有的，下面我们来看看Spark streaming。

### Spark streaming
#### Spark streaming采用小批量的方式，提高了吞吐性能：
![](https://user-gold-cdn.xitu.io/2019/4/1/169d68e3171ffeb4)
#### 这里我们简单展示Spark streaming的运行机制，主要是与Storm做下对比。Spark streaming批量读取数据源中的数据，然后把每个batch转化成内部的RDD。Spark streaming以batch为单位进行计算（默认1s产生一个batch），而不是以record为单位，大大减少了ack所需的开销，显著提高了吞吐。
### 既然我们上边说到Storm会有“不重不丢”的问题，那么我们来看一下SparkStreamimg是如何解决这个问题的。不过，在了解这个问题之前，我们需要来了解一些概念。
#### 1.at most once：最多消费一次，会存在数据丢失
#### 2.at least once：最少消费一次，保证数据不丢，但是有可能重复消费
#### 3.exactly once：精确一次，无论何种情况下，数据都只会消费一次，这是我们最希望看到的结果
#### 大部分流式系统都提供了at most once和at least once功能，但不是所有系统都能提供exactly once。
#### 我们先看看Spark streaming的at least once是如何实现的，Spark streaming的每个batch可以看做是一个Spark任务，receiver会先将数据写入WAL，保证receiver宕机时，从数据源获取的数据能够从日志中恢复（注意这里，早期的Spark streaming的receiver存在重复接收数据的情况），并且依赖RDD实现内部的exactly once（可以简单的理解采用批量计算的方式来实现），Spark保存着RDD之间的依赖关系，保证RDD计算失败时，可以通过上游RDD进行重新计算（RDD如何实现容错这里就不解释了，可以自行查资料）。
### SparkStreaming中exactly once猜想
![](https://user-gold-cdn.xitu.io/2019/4/1/169d6b0f7af8668c)
#### Spark streaming的RDD机制只能保证内部计算exactly once（图中的1），但这是不够的，回想一下刚才Storm的例子，假如某个batch中，sink处一部分数据已经入库，这时候某个sink节点宕机，导致该节点处理的数据重复输出（图中的3，Storm处已经解释过了）。还有另一种情况就是receiver处重复接收数据（图中的2），我们看一下receiver重复接收数据的情况：
![](https://user-gold-cdn.xitu.io/2019/4/1/169d7b004ff5b6fa)
#### 假如receiverA目前从kafka读到pos=100的记录，并且已经持久化到HDFS，但是由于网络延迟没有及时更新pos，此时receiverA宕机了，receiverB接管A的数据，并且后续的任务还会从pos=100处重新读取，导致重复消费。造成这种情况的主要原因就是：receiver处数据消费和Kafka中position的更新没有做到原子性。
### 根据上面的讨论，可以得出：一个流式系统如果要做到exactly once，必须满足3点：
#### 1.receiver处保证exactly once
#### 2.流式系统自身保证exactly once
#### 3.sink处保证exactly once
#### 这里数据源采用Kafka举例是因为Kafka作为目前主流的分布式消息队列，比较有代表性。Kafka consumer的position可以保存在ZK或者Kafka中，也可以由consumer自己来保存。前者的话就可能存在数据消费和position更新不一致的问题（因为无法保证原子性，也是之前Spark streaming采用的方式），而采用后者的话，consumer可以采用事务更新的方式（写本地或者采用事务的方式写数据库），保证数据消费和position更新的原子性，从而实现exactly once。
### SparkStreaming是如何实现exactly once
#### Spark streaming1.3版本新添加了Kafka Direct API来实现数据接收的exactly once，本质上就是上面提到的后者，Spark streaming自己维护position，streaming的worker直接从Kafka读取数据，position由Spark streaming管理，不再依赖ZK保存，同时保证数据消费和更新position的原子性，从而实现exactly once。并且新的方式已经不再需要receiver持久化数据，因为Kafka本身就支持数据持久化，可以避免receiver处持久化数据的开销，实现exactly once的同时也提高了性能。而sink处的exactly once的实现则视外部系统而定，比如文件系统本身就支持幂等（同一个操作执行多次，不会改变之前的结果），同时Spark streaming也提供了api，用户可以自己实现sink处的事务更新，receiver、sink和Spark streaming三者结合起来才能实现了真正的exactly once。
## Flink
#### 敬请期待~不过有个大哥，大家可以看看关于Flink的[老司机](http://www.54tianzhisheng.cn/tags/Flink/)